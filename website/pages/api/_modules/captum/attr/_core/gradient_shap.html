
<script type="text/javascript" id="documentation_options" data-url_root="./"
  src="/js/documentation_options.js"></script>
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/underscore.js"></script>
<script type="text/javascript" src="/js/doctools.js"></script>
<script type="text/javascript" src="/js/language_data.js"></script>
<script type="text/javascript" src="/js/searchtools.js"></script>
<div class="sphinx"><div class="document">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body" role="main">
<h1>Source code for captum.attr._core.gradient_shap</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">.._utils.attribution</span> <span class="k">import</span> <span class="n">GradientAttribution</span>
<span class="kn">from</span> <span class="nn">.._utils.common</span> <span class="k">import</span> <span class="n">_format_attributions</span>

<span class="kn">from</span> <span class="nn">.noise_tunnel</span> <span class="k">import</span> <span class="n">NoiseTunnel</span>


<div class="viewcode-block" id="GradientShap"><a class="viewcode-back" href="../../../../gradient_shap.html#captum.attr._core.gradient_shap.GradientShap">[docs]</a><span class="k">class</span> <span class="nc">GradientShap</span><span class="p">(</span><span class="n">GradientAttribution</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forward_func</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">        Args:</span>

<span class="sd">            forward_func (function): The forward function of the model or</span>
<span class="sd">                       any modification of it</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">forward_func</span><span class="p">)</span>

<div class="viewcode-block" id="GradientShap.attribute"><a class="viewcode-back" href="../../../../gradient_shap.html#captum.attr._core.gradient_shap.GradientShap.attribute">[docs]</a>    <span class="k">def</span> <span class="nf">attribute</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">baselines</span><span class="p">,</span>
        <span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">stdevs</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">additional_forward_args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">        Implements gradient SHAP based on the implementation from SHAP's primary</span>
<span class="sd">        author. For reference, please, view:</span>

<span class="sd">        https://github.com/slundberg/shap/#deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models</span>

<span class="sd">        A Unified Approach to Interpreting Model Predictions</span>
<span class="sd">        http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions</span>

<span class="sd">        GradientShap approximates SHAP values by computing the expectations of</span>
<span class="sd">        gradients by randomly sampling from the distribution of baselines/references.</span>
<span class="sd">        It adds white noise to each input sample `n_samples` times, selects a</span>
<span class="sd">        random point along the path between baseline and input, and computes the</span>
<span class="sd">        gradient of outputs with respect to those selected random points.</span>
<span class="sd">        The final SHAP values represent the expected values of</span>
<span class="sd">        gradients * (inputs - baselines).</span>

<span class="sd">        GradientShap makes an assumption that the input features are independent</span>
<span class="sd">        and that there is a linear relationship between current inputs and the</span>
<span class="sd">        baselines/references. Under those assumptions, SHAP value can be</span>
<span class="sd">        approximated as the expectation of gradients that are computed for randomly</span>
<span class="sd">        generated `n_samples` input samples after adding gaussian noise `n_samples`</span>
<span class="sd">        times to each input for different baselines/references.</span>

<span class="sd">        In some sense it can be viewed as an approximation of integrated gradients</span>
<span class="sd">        by computing the expectations of gradients for different baselines.</span>

<span class="sd">        Current implementation uses Smoothgrad from `NoiseTunnel` in order to</span>
<span class="sd">        randomly draw samples from the distribution of baselines, add noise to input</span>
<span class="sd">        samples and compute the expectation (smoothgrad).</span>

<span class="sd">        Args:</span>

<span class="sd">            inputs (tensor or tuple of tensors):  Input for which integrated</span>
<span class="sd">                        gradients are computed. If forward_func takes a single</span>
<span class="sd">                        tensor as input, a single input tensor should be provided.</span>
<span class="sd">                        If forward_func takes multiple tensors as input, a tuple</span>
<span class="sd">                        of the input tensors should be provided. It is assumed</span>
<span class="sd">                        that for all given input tensors, dimension 0 corresponds</span>
<span class="sd">                        to the number of examples, and if mutliple input tensors</span>
<span class="sd">                        are provided, the examples must be aligned appropriately.</span>
<span class="sd">            baselines (tensor or tuple of tensors, optional):  Baselines define</span>
<span class="sd">                        the starting point from which expectation is computed.</span>
<span class="sd">                        If inputs is a single tensor, baselines must also be a</span>
<span class="sd">                        single tensor.</span>
<span class="sd">                        If inputs is a tuple of tensors, baselines must also be</span>
<span class="sd">                        a tuple of tensors, with the same number of tensors as</span>
<span class="sd">                        the inputs. The first dimension in baseline tensors</span>
<span class="sd">                        defines the distribution from which we randomly draw</span>
<span class="sd">                        samples. All other dimensions starting after</span>
<span class="sd">                        the first dimension should match with the inputs'</span>
<span class="sd">                        dimensions after the first dimension. It is recommended that</span>
<span class="sd">                        the number of samples in the baselines' tensors is larger</span>
<span class="sd">                        than one.</span>
<span class="sd">                        Default: zero tensor for each input tensor</span>
<span class="sd">            target (int, optional):  Output index for which gradient is computed</span>
<span class="sd">                        (for classification cases, this is the target class).</span>
<span class="sd">                        If the network returns a scalar value per example,</span>
<span class="sd">                        no target index is necessary. (Note: Tuples for multi</span>
<span class="sd">                        -dimensional output indices will be supported soon.)</span>
<span class="sd">            additional_forward_args (tuple, optional): If the forward function</span>
<span class="sd">                        requires additional arguments other than the inputs for</span>
<span class="sd">                        which attributions should not be computed, this argument</span>
<span class="sd">                        can be provided. It can contain a tuple of ND tensors or</span>
<span class="sd">                        any arbitrary python type of any shape.</span>
<span class="sd">                        In case of the ND tensor the first dimension of the</span>
<span class="sd">                        tensor must correspond to the batch size. It will be</span>
<span class="sd">                        repeated for each `n_steps` for each randomly generated</span>
<span class="sd">                        input sample.</span>
<span class="sd">                        Note that the gradients are not computed with respect</span>
<span class="sd">                        to these arguments.</span>
<span class="sd">                        Default: None</span>
<span class="sd">            n_samples (int, optional):  The number of randomly generated examples</span>
<span class="sd">                        per sample in the input batch. Random examples are</span>
<span class="sd">                        generated by adding gaussian random noise to each sample.</span>
<span class="sd">                        Default: `5` if `n_samples` is not provided.</span>

<span class="sd">        Returns:</span>

<span class="sd">            attributions (tensor or tuple of tensors): Attribution score</span>
<span class="sd">                        computed based on GradientSHAP with respect</span>
<span class="sd">                        to each input feature. Attributions will always be</span>
<span class="sd">                        the same size as the provided inputs, with each value</span>
<span class="sd">                        providing the attribution of the corresponding input index.</span>
<span class="sd">                        If a single tensor is provided as inputs, a single tensor is</span>
<span class="sd">                        returned. If a tuple is provided for inputs, a tuple of</span>
<span class="sd">                        corresponding sized tensors is returned.</span>
<span class="sd">            delta (float): This is computed using the property that the total</span>
<span class="sd">                        sum of forward_func(inputs) - forward_func(baselines)</span>
<span class="sd">                        must be very colse to the total sum of the attributions</span>
<span class="sd">                        based on GradientSHAP.</span>

<span class="sd">            Examples::</span>

<span class="sd">                &gt;&gt;&gt; # ImageClassifier takes a single input tensor of images Nx3x32x32,</span>
<span class="sd">                &gt;&gt;&gt; # and returns an Nx10 tensor of class probabilities.</span>
<span class="sd">                &gt;&gt;&gt; net = ImageClassifier()</span>
<span class="sd">                &gt;&gt;&gt; gradient_shap = GradientShap(net)</span>
<span class="sd">                &gt;&gt;&gt; input = torch.randn(3, 3, 32, 32, requires_grad=True)</span>
<span class="sd">                &gt;&gt;&gt; # choosing baselines randomly</span>
<span class="sd">                &gt;&gt;&gt; baselines = torch.randn(20, 3, 32, 32)</span>
<span class="sd">                &gt;&gt;&gt; # Computes gradient shap for the input</span>
<span class="sd">                &gt;&gt;&gt; # Attribution size matches input size: 3x3x32x32</span>
<span class="sd">                &gt;&gt;&gt; attribution, delta = gradient_shap.attribute(input, baselines,</span>
<span class="sd">                                                                 target=5)</span>

<span class="sd">        """</span>
        <span class="n">input_min_baseline_x_grad</span> <span class="o">=</span> <span class="n">InputBaselineXGradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_func</span><span class="p">)</span>

        <span class="n">nt</span> <span class="o">=</span> <span class="n">NoiseTunnel</span><span class="p">(</span><span class="n">input_min_baseline_x_grad</span><span class="p">)</span>
        <span class="n">attributions</span> <span class="o">=</span> <span class="n">nt</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">nt_type</span><span class="o">=</span><span class="s2">"smoothgrad"</span><span class="p">,</span>
            <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
            <span class="n">stdevs</span><span class="o">=</span><span class="n">stdevs</span><span class="p">,</span>
            <span class="n">draw_baseline_from_distrib</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">baselines</span><span class="o">=</span><span class="n">baselines</span><span class="p">,</span>
            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
            <span class="n">additional_forward_args</span><span class="o">=</span><span class="n">additional_forward_args</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_convergence_delta</span><span class="p">(</span>
            <span class="n">attributions</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attributions</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">attributions</span><span class="p">,),</span>
            <span class="n">baselines</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">additional_forward_args</span><span class="o">=</span><span class="n">additional_forward_args</span><span class="p">,</span>
            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
            <span class="n">is_multi_baseline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">attributions</span><span class="p">,</span> <span class="n">delta</span></div>

    <span class="k">def</span> <span class="nf">_has_convergence_delta</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="InputBaselineXGradient"><a class="viewcode-back" href="../../../../gradient_shap.html#captum.attr._core.gradient_shap.InputBaselineXGradient">[docs]</a><span class="k">class</span> <span class="nc">InputBaselineXGradient</span><span class="p">(</span><span class="n">GradientAttribution</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forward_func</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">"""</span>
<span class="sd">        Args:</span>

<span class="sd">            forward_func (function): The forward function of the model or</span>
<span class="sd">                       any modification of it</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">forward_func</span><span class="p">)</span>

<div class="viewcode-block" id="InputBaselineXGradient.attribute"><a class="viewcode-back" href="../../../../gradient_shap.html#captum.attr._core.gradient_shap.InputBaselineXGradient.attribute">[docs]</a>    <span class="k">def</span> <span class="nf">attribute</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">baselines</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">additional_forward_args</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
        <span class="k">def</span> <span class="nf">scale_input</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">baseline</span><span class="p">,</span> <span class="n">rand_coefficient</span><span class="p">):</span>
            <span class="c1"># batch size</span>
            <span class="n">bsz</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">inp_shape_wo_bsz</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">inp_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">inp_shape_wo_bsz</span><span class="p">))</span>

            <span class="c1"># expand and reshape the indices</span>
            <span class="n">rand_coefficient</span> <span class="o">=</span> <span class="n">rand_coefficient</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">inp_shape</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

            <span class="n">input_baseline_scaled</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">rand_coefficient</span> <span class="o">*</span> <span class="nb">input</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rand_coefficient</span><span class="p">)</span> <span class="o">*</span> <span class="n">baseline</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">input_baseline_scaled</span>

        <span class="c1"># Keeps track whether original input is a tuple or not before</span>
        <span class="c1"># converting it into a tuple.</span>
        <span class="n">is_inputs_tuple</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>

        <span class="n">rand_coefficient</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
            <span class="n">device</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">input_baseline_scaled</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">scale_input</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">baseline</span><span class="p">,</span> <span class="n">rand_coefficient</span><span class="p">)</span>
            <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">baseline</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">baselines</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_func</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">forward_func</span><span class="p">,</span> <span class="n">input_baseline_scaled</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">additional_forward_args</span>
        <span class="p">)</span>

        <span class="n">input_baseline_diffs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="nb">input</span> <span class="o">-</span> <span class="n">baseline</span> <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">baseline</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">baselines</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">attributions</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">input_baseline_diff</span> <span class="o">*</span> <span class="n">grad</span>
            <span class="k">for</span> <span class="n">input_baseline_diff</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_baseline_diffs</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">_format_attributions</span><span class="p">(</span><span class="n">is_inputs_tuple</span><span class="p">,</span> <span class="n">attributions</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_has_convergence_delta</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span></div>
</pre></div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">Captum</a></h1>
<h3>Navigation</h3>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../saliency.html">Saliency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deep_lift.html">DeepLift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deep_lift_shap.html">DeepLiftShap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../gradient_shap.html">GradientShap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../input_x_gradient.html">InputXGradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../integrated_gradients.html">IntegratedGradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../noise_tunnel.html">NoiseTunnel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../base.html">base</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../neuron.html">neuron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../layer.html">layer</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
<li><a href="../../../../index.html">Documentation overview</a><ul>
<li><a href="../../../index.html">Module code</a><ul>
</ul></li>
</ul></li>
</ul>
</div>
<div id="searchbox" role="search" style="display: none">
<h3 id="searchlabel">Quick search</h3>
<div class="searchformwrapper">
<form action="../../../../search.html" class="search" method="get">
<input aria-labelledby="searchlabel" name="q" type="text"/>
<input type="submit" value="Go"/>
</form>
</div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
</div>
</div>
<div class="clearer"></div>
</div></div>