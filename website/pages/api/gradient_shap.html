
<script type="text/javascript" id="documentation_options" data-url_root="./"
  src="/js/documentation_options.js"></script>
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/underscore.js"></script>
<script type="text/javascript" src="/js/doctools.js"></script>
<script type="text/javascript" src="/js/language_data.js"></script>
<script type="text/javascript" src="/js/searchtools.js"></script>
<div class="sphinx"><div class="document">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body" role="main">
<div class="section" id="module-captum.attr._core.gradient_shap">
<span id="gradientshap"></span><h1>GradientShap<a class="headerlink" href="#module-captum.attr._core.gradient_shap" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="captum.attr._core.gradient_shap.GradientShap">
<em class="property">class </em><code class="sig-prename descclassname">captum.attr._core.gradient_shap.</code><code class="sig-name descname">GradientShap</code><span class="sig-paren">(</span><em class="sig-param">forward_func</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/captum/attr/_core/gradient_shap.html#GradientShap"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#captum.attr._core.gradient_shap.GradientShap" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>forward_func</strong> (<em>function</em>) – The forward function of the model or
any modification of it</p>
</dd>
</dl>
<dl class="method">
<dt id="captum.attr._core.gradient_shap.GradientShap.attribute">
<code class="sig-name descname">attribute</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">baselines</em>, <em class="sig-param">n_samples=50</em>, <em class="sig-param">stdevs=0.0</em>, <em class="sig-param">target=None</em>, <em class="sig-param">additional_forward_args=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/captum/attr/_core/gradient_shap.html#GradientShap.attribute"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#captum.attr._core.gradient_shap.GradientShap.attribute" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements gradient SHAP based on the implementation from SHAP’s primary
author. For reference, please, view:</p>
<p><a class="reference external" href="https://github.com/slundberg/shap/#deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models">https://github.com/slundberg/shap/#deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models</a></p>
<p>A Unified Approach to Interpreting Model Predictions
<a class="reference external" href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions">http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions</a></p>
<p>GradientShap approximates SHAP values by computing the expectations of
gradients by randomly sampling from the distribution of baselines/references.
It adds white noise to each input sample <cite>n_samples</cite> times, selects a
random point along the path between baseline and input, and computes the
gradient of outputs with respect to those selected random points.
The final SHAP values represent the expected values of
gradients * (inputs - baselines).</p>
<p>GradientShap makes an assumption that the input features are independent
and that there is a linear relationship between current inputs and the
baselines/references. Under those assumptions, SHAP value can be
approximated as the expectation of gradients that are computed for randomly
generated <cite>n_samples</cite> input samples after adding gaussian noise <cite>n_samples</cite>
times to each input for different baselines/references.</p>
<p>In some sense it can be viewed as an approximation of integrated gradients
by computing the expectations of gradients for different baselines.</p>
<p>Current implementation uses Smoothgrad from <cite>NoiseTunnel</cite> in order to
randomly draw samples from the distribution of baselines, add noise to input
samples and compute the expectation (smoothgrad).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tensor</em><em> or </em><em>tuple of tensors</em>) – Input for which integrated
gradients are computed. If forward_func takes a single
tensor as input, a single input tensor should be provided.
If forward_func takes multiple tensors as input, a tuple
of the input tensors should be provided. It is assumed
that for all given input tensors, dimension 0 corresponds
to the number of examples, and if mutliple input tensors
are provided, the examples must be aligned appropriately.</p></li>
<li><p><strong>baselines</strong> (<em>tensor</em><em> or </em><em>tuple of tensors</em><em>, </em><em>optional</em>) – Baselines define
the starting point from which expectation is computed.
If inputs is a single tensor, baselines must also be a
single tensor.
If inputs is a tuple of tensors, baselines must also be
a tuple of tensors, with the same number of tensors as
the inputs. The first dimension in baseline tensors
defines the distribution from which we randomly draw
samples. All other dimensions starting after
the first dimension should match with the inputs’
dimensions after the first dimension. It is recommended that
the number of samples in the baselines’ tensors is larger
than one.
Default: zero tensor for each input tensor</p></li>
<li><p><strong>target</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Output index for which gradient is computed
(for classification cases, this is the target class).
If the network returns a scalar value per example,
no target index is necessary. (Note: Tuples for multi
-dimensional output indices will be supported soon.)</p></li>
<li><p><strong>additional_forward_args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – If the forward function
requires additional arguments other than the inputs for
which attributions should not be computed, this argument
can be provided. It can contain a tuple of ND tensors or
any arbitrary python type of any shape.
In case of the ND tensor the first dimension of the
tensor must correspond to the batch size. It will be
repeated for each <cite>n_steps</cite> for each randomly generated
input sample.
Note that the gradients are not computed with respect
to these arguments.
Default: None</p></li>
<li><p><strong>n_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – The number of randomly generated examples
per sample in the input batch. Random examples are
generated by adding gaussian random noise to each sample.
Default: <cite>5</cite> if <cite>n_samples</cite> is not provided.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Attribution score</dt><dd><p>computed based on GradientSHAP with respect
to each input feature. Attributions will always be
the same size as the provided inputs, with each value
providing the attribution of the corresponding input index.
If a single tensor is provided as inputs, a single tensor is
returned. If a tuple is provided for inputs, a tuple of
corresponding sized tensors is returned.</p>
</dd>
<dt>delta (float): This is computed using the property that the total</dt><dd><p>sum of forward_func(inputs) - forward_func(baselines)
must be very colse to the total sum of the attributions
based on GradientSHAP.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># ImageClassifier takes a single input tensor of images Nx3x32x32,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># and returns an Nx10 tensor of class probabilities.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">ImageClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_shap</span> <span class="o">=</span> <span class="n">GradientShap</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># choosing baselines randomly</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">baselines</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Computes gradient shap for the input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Attribution size matches input size: 3x3x32x32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attribution</span><span class="p">,</span> <span class="n">delta</span> <span class="o">=</span> <span class="n">gradient_shap</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">baselines</span><span class="p">,</span>
<span class="go">                                                 target=5)</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>attributions (tensor or tuple of tensors)</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="captum.attr._core.gradient_shap.InputBaselineXGradient">
<em class="property">class </em><code class="sig-prename descclassname">captum.attr._core.gradient_shap.</code><code class="sig-name descname">InputBaselineXGradient</code><span class="sig-paren">(</span><em class="sig-param">forward_func</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/captum/attr/_core/gradient_shap.html#InputBaselineXGradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#captum.attr._core.gradient_shap.InputBaselineXGradient" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>forward_func</strong> (<em>function</em>) – The forward function of the model or
any modification of it</p>
</dd>
</dl>
<dl class="method">
<dt id="captum.attr._core.gradient_shap.InputBaselineXGradient.attribute">
<code class="sig-name descname">attribute</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">baselines=None</em>, <em class="sig-param">target=None</em>, <em class="sig-param">additional_forward_args=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/captum/attr/_core/gradient_shap.html#InputBaselineXGradient.attribute"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#captum.attr._core.gradient_shap.InputBaselineXGradient.attribute" title="Permalink to this definition">¶</a></dt>
<dd><p>This method computes and returns the attribution values for each input tensor
Deriving classes are responsible for implementing its logic accordingly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – A single high dimensional input tensor or a tuple of them.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Attribution values for each input vector. The</dt><dd><p><cite>attributions</cite> have the dimensionality of inputs
for standard attribution derived classes and the
dimensionality of the given tensor for layer attributions.</p>
</dd>
</dl>
<p>others ?</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>attributions</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Captum</a></h1>
<h3>Navigation</h3>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="saliency.html">Saliency</a></li>
<li class="toctree-l1"><a class="reference internal" href="deep_lift.html">DeepLift</a></li>
<li class="toctree-l1"><a class="reference internal" href="deep_lift_shap.html">DeepLiftShap</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">GradientShap</a></li>
<li class="toctree-l1"><a class="reference internal" href="input_x_gradient.html">InputXGradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="integrated_gradients.html">IntegratedGradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="noise_tunnel.html">NoiseTunnel</a></li>
<li class="toctree-l1"><a class="reference internal" href="base.html">base</a></li>
<li class="toctree-l1"><a class="reference internal" href="neuron.html">neuron</a></li>
<li class="toctree-l1"><a class="reference internal" href="layer.html">layer</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
<li><a href="index.html">Documentation overview</a><ul>
<li>Previous: <a href="deep_lift_shap.html" title="previous chapter">DeepLiftShap</a></li>
<li>Next: <a href="input_x_gradient.html" title="next chapter">InputXGradient</a></li>
</ul></li>
</ul>
</div>
<div id="searchbox" role="search" style="display: none">
<h3 id="searchlabel">Quick search</h3>
<div class="searchformwrapper">
<form action="search.html" class="search" method="get">
<input aria-labelledby="searchlabel" name="q" type="text"/>
<input type="submit" value="Go"/>
</form>
</div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
</div>
</div>
<div class="clearer"></div>
</div></div>